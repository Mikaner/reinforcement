{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOWgGnmTAVVrqweBVmn+AoU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mikaner/reinforcement/blob/main/DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nA1oUq8_r1o",
        "outputId": "d419f96a-70c0-4fc8-a20c-049c8a6c4da4"
      },
      "source": [
        "!apt-get -qq -y install libcusparse8.0 libnvrtc8.0 libnvtoolsext1 > /dev/null\n",
        "!ln -snf /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so.8.0 /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so\n",
        "!apt -qq install xvfb freeglut3-dev ffmpeg> /dev/null\n",
        "!pip -q install gym\n",
        "!pip -q install JSAnimation\n",
        "!pip -q install pyglet\n",
        "!pip -q install pyopengl\n",
        "!pip -q install pyvirtualdisplay"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Unable to locate package libcusparse8.0\n",
            "E: Couldn't find any package by glob 'libcusparse8.0'\n",
            "E: Couldn't find any package by regex 'libcusparse8.0'\n",
            "E: Unable to locate package libnvrtc8.0\n",
            "E: Couldn't find any package by glob 'libnvrtc8.0'\n",
            "E: Couldn't find any package by regex 'libnvrtc8.0'\n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "  Building wheel for JSAnimation (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vx2W14wg_w9s"
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1024, 768))\n",
        "display.start()\n",
        "import os\n",
        "os.environ[\"DISPLAY\"] = f\":{display.display}\"\n",
        "# https://github.com/ponty/PyVirtualDisplay/issues/54"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6gigdRH_11H"
      },
      "source": [
        "# 動画の描画関数の宣言\n",
        "# 参考URL: http://nbviewer.jupyter.org/github/patrickmineault/xcorr-notebooks/blob/master/Render%20OpenAI%20gym%20as%20GIF.ipynb\n",
        "from JSAnimation.IPython_display import display_animation\n",
        "from matplotlib import animation\n",
        "#from IPython.display import display\n",
        "from IPython.display import HTML\n",
        "\n",
        "def make_anim(frames):\n",
        "    plt.figure(figsize=(frames[0].shape[1]/72.0, frames[0].shape[0]/72.0), \n",
        "               dpi=72)\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "    \n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames),\n",
        "                                   interval=50)\n",
        "    return anim\n",
        "\n",
        "def save_frames_as_gif(frames):\n",
        "    \"\"\"\n",
        "    DISPLAYs a list of frames as a gif, with controls\n",
        "    \"\"\"\n",
        "    \n",
        "    anim = make_anim(frames)\n",
        "    anim.save('movie_cartpole_DQN.mp4')\n",
        "    #display(display_animation(anim, default_mode='loop'))\n",
        "    return anim.to_jshtml()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ea7sU7mMf4rx"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import gym"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dpfvo2P_kxJ",
        "outputId": "233f3751-5297-456e-e1cb-c0a2c2389618"
      },
      "source": [
        "# namedtupleの実装\n",
        "# namedtupleにて値とフィールド名をペアで格納できる\n",
        "# 以下は使用例\n",
        "\n",
        "from collections import namedtuple\n",
        "\n",
        "Tr = namedtuple('tr', ('name_a', 'value_b'))\n",
        "Tr_object = Tr('名前Aです', 100)\n",
        "\n",
        "print(Tr_object)\n",
        "print(Tr_object.value_b)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tr(name_a='名前Aです', value_b=100)\n",
            "100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Utq10Ue4AvaV"
      },
      "source": [
        "# namedtupleを生成\n",
        "from collections import namedtuple\n",
        "\n",
        "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDVlVO79BLtr"
      },
      "source": [
        "ENV = 'CartPole-v0'\n",
        "GAMMA = 0.99\n",
        "MAX_STEPS = 200\n",
        "NUM_EPISODES = 500"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9k9_Z1koBUvM"
      },
      "source": [
        "# ミニバッチ学習を実現するために\n",
        "# 経験を保存するメモリクラスを定義します\n",
        "\n",
        "class ReplayMemory:\n",
        "    def __init__(self, CAPACITY):\n",
        "        self.capacity = CAPACITY\n",
        "        self.memory = []\n",
        "        self.index = 0\n",
        "\n",
        "    def push(self, state, action, state_next, reward):\n",
        "        '''transition = (state, action, state_next, reward)をメモリに保存する'''\n",
        "\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None) # メモリが満タンでないときは足す\n",
        "            \n",
        "        # namedtupleのTransitionを使用し、値とフィールド名をペアにして保存します\n",
        "        self.memory[self.index] = Transition(state, action, state_next, reward)\n",
        "\n",
        "        self.index = (self.index + 1) % self.capacity # 保存するindexを1つずらす(最大の場合は最初に上書き)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        '''batch_size分だけ、ランダムに保存内容を取り出す'''\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        '''関数lenに対して、現在の変数memoryの長さを返す'''\n",
        "        return len(self.memory)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9S5orRy_LT17"
      },
      "source": [
        "# エージェントが持つ脳となるクラス。DQNを実行する。\n",
        "# Q関数をディープラーニングのネットワークをクラスとして定義\n",
        "\n",
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "CAPACITY = 10000  # メモリの最大値(メモリが足りなくなるなんてことがあるのかしら)\n",
        "\n",
        "class Brain:\n",
        "    def __init__(self, num_states, num_actions):\n",
        "        self.num_actions = num_actions # CartPoleの行動(右に左に押す)の2を取得\n",
        "\n",
        "        # 経験を記憶するメモリオブジェクトを生成\n",
        "        self.memory = ReplayMemory(CAPACITY)\n",
        "        \n",
        "        # ニューラルネットワークを構築\n",
        "        self.model = nn.Sequential()\n",
        "        self.model.add_module('fc1', nn.Linear(num_states, 32))\n",
        "        self.model.add_module('relu1', nn.ReLU())\n",
        "        self.model.add_module('fc2', nn.Linear(32, 32))\n",
        "        self.model.add_module('relu2', nn.ReLU())\n",
        "        self.model.add_module('fc3', nn.Linear(32, num_actions))\n",
        "\n",
        "        print(self.model)\n",
        "\n",
        "        # 最適化手法の設定\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.0001)\n",
        "\n",
        "    def replay(self):\n",
        "        '''Experience Replayでネットワークの結合パラメータを学習'''\n",
        "\n",
        "        # -------------------------------------------------\n",
        "        # 1. メモリサイズの確認\n",
        "        # -------------------------------------------------\n",
        "        # 1.1 メモリサイズがミニバッチより小さい間は何もしない\n",
        "        if len(self.memory) < BATCH_SIZE:\n",
        "            return\n",
        "        \n",
        "        # -------------------------------------------------\n",
        "        # 2. ミニバッチの作成\n",
        "        # -------------------------------------------------\n",
        "        # 2.1 メモリからミニバッチ分のデータを取り出す\n",
        "        transitions = self.memory.sample(BATCH_SIZE)\n",
        "\n",
        "        # 2.2 各変数をミニバッチに対応する形に変形\n",
        "        # transitionsは1stepごとの(state, action, state_next, reward)が、BATCH_SIZE分格納されている\n",
        "        # つまり、(state, action, state_next, reward)×BATCH_SIZE\n",
        "        # これをミニバッチにしたい。つまり\n",
        "        # (state×BATCH_SIZE, action×BATCH_SIZE, state_next×BATCH_SIZE, reward×BATCH_SIZE)にする\n",
        "        # ちなみにそれぞれは「各要素の数」を表してそうですね。\n",
        "        batch = Transition(*zip(*transitions))\n",
        "        # 例えば、\n",
        "        # step (state, action, state_next, reward)が\n",
        "        #    1 (    2,      0,          1,      1)\n",
        "        #    2 (    1,      1,          2,      0)となっている場合\n",
        "        # batch内は\n",
        "        # ([2,1], [0,1], [1,2], [1,0])となっている感じかしら。\n",
        "\n",
        "        # 2.3 各変数の要素をミニバッチに対応する形に変形する。\n",
        "        # 例えばstateの場合、[torch.FloatTensor of size 1×4]がBATCH_SIZE分並んでいる(カートの位置、速度、棒の角度、角速度)\n",
        "        # のですが、それを torch.FloatTensor of size BATCH_SIZE×4に変換します。←要は1をBATCH_SIZE数に変更するということですね\n",
        "        # catはConcatenates(結合)のことです。\n",
        "        state_batch = torch.cat(batch.state)\n",
        "        action_batch = torch.cat(batch.action)\n",
        "        reward_batch = torch.cat(batch.reward)\n",
        "        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
        "        # 要はstateが\n",
        "        # ([[20, 3, 1/2pi, 0.5], [22, 2, 3/4pi, 1]])と並んでいるのを\n",
        "        # ([[20, 22], [3, 2], [1/2pi, 3/4pi], [0.5, 1]])みたいに変えるって事かな\n",
        "\n",
        "        # -------------------------------------------------\n",
        "        # 3. 教師信号となるQ(s_t, a_t)値を求める\n",
        "        # -------------------------------------------------\n",
        "        # 3.1 ネットワークを推論モードに切り替える\n",
        "        self.model.eval()\n",
        "\n",
        "        # 3.2 ネットワークが出力したQ(s_t, a_t)を求める\n",
        "        # self.model(state_batch)は、右左の両方のQ値を出力しており\n",
        "        # [torch.FloatTensor of size BATCH_SIZE×2]になっている\n",
        "        # ここから実行したアクションa_tに対応するQ値を求めるため、action_batchで行った行動a_tが右か左かのindexを求め\n",
        "        # それに対応するQ値をgatherで引っ張り出す。 ← gatherで引っ張り出すって具体的な挙動は何をする関数なのだろうか ← おそらくgatherでQ値を引っ張り出すって意味だと思われる。具体的な挙動は下セルに書いた。\n",
        "        # ようわからんけど、state_batchの時に出力した値からgather使ってどっちを選んでるかを判断するみたいな感じかしら\n",
        "        # いや、state_batchのデータで推論して、それのアクション(左右)とaction_batchと対応付けているんじゃないかな\n",
        "        # まあ簡単に言うと、action_batchで判断したインデックスのQ値を求めてるっぽい\n",
        "\n",
        "        state_action_values = self.model(state_batch).gather(1, action_batch)\n",
        "\n",
        "        print(action_batch)\n",
        "        print(state_action_values)\n",
        "\n",
        "        # 3.3 max{Q(s_t+1, a)}値を求める。ただし次の状態があるかに注意\n",
        "\n",
        "        # cartpoleがdoneになっておらず、next_stateがあるかをチェックするインデックスマスクを作成\n",
        "        # ここlambda使ったワンライナーの悪行風味\n",
        "        non_final_mask = torch.ByteTensor(\n",
        "            tuple(map(lambda s: s is not None, batch.next_state)))\n",
        "        # まずは全部0にしておく\n",
        "        next_state_values = torch.zeros(BATCH_SIZE)\n",
        "\n",
        "        # 次の状態があるindexの最大Q値を求める\n",
        "        # 出力にアクセスし、max(1)で列方向の最大値の[値、index]を求めます\n",
        "        # そしてそのQ値(上で取り出した出力[値、index]の0番目)を出力します。\n",
        "        # detachでその値を取り出します。\n",
        "        # えっと？つまり？次が存在するもののマックス値を求めてるってことか？\n",
        "        # いや、正確に言うと、次が存在する状態のQ値の集合(?)のそれぞれの最大を取得しているみたい。\n",
        "        next_state_values[non_final_mask] = self.model(non_final_next_states).max(1)[0].detach()\n",
        "        \n",
        "        # 3.4 教師となるQ(s_t, a_t)値を、Q学習の式から求める (要は誤差を求めるわけですわ)\n",
        "        expected_state_action_values = reward_batch + GAMMA * next_state_values\n",
        "\n",
        "        # -------------------------------------------------\n",
        "        # 4. 結合パラメータの更新\n",
        "        # -------------------------------------------------\n",
        "        # 4.1 ネットワークを訓練モードに切り替える\n",
        "        self.model.train()\n",
        "\n",
        "        # 4.2 損失関数を計算する (smooth_l1_lossはHuberloss) ← Huberlossとは\n",
        "        # expected_state_action_valusは\n",
        "        # sizeが[minibatch]になっているので、unsqueezeで[minibatch × 1]へ\n",
        "        loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze (1)) # この(1)って引数？ 間のスペースいる？\n",
        "\n",
        "        # 4.3 結合パラメータを更新する\n",
        "        self.optimizer.zero_grad() # 勾配をリセット ← なんでリセットするんだっけ。前の更新したデータが残ってるからか。\n",
        "        loss.backward() # バックプロパゲーションを計算\n",
        "        self.optimizer.step() # 結合パラメータを更新\n",
        "\n",
        "    def decide_action(self, state, episode):\n",
        "        ''' 現在の状況に応じて、行動を決定する '''\n",
        "        # ε-greedy法で徐々に最適行動のみを採用する\n",
        "        epsilon = 0.5 * (1 / (episode + 1))\n",
        "\n",
        "        if epsilon <= np.random.uniform(0, 1):\n",
        "            self.model.eval() # ネットワークを推論モードに切り替える\n",
        "            with torch.no_grad():\n",
        "                action = self.model(state).max(1)[1].view(1, 1)\n",
        "            # ネットワークの出力の最大値のindexを取り出します = max(1)[1]\n",
        "            # .view(1, 1)は[torch.LongTensor of size 1] を size 1x1 に変換します\n",
        "        \n",
        "        else:\n",
        "            # 0, 1の行動をランダムに返す\n",
        "            action = torch.LongTensor([[random.randrange(self.num_actions)]]) # 0, 1の行動をランダムに返す\n",
        "            # actionは[torch.LongTensor of size 1x1]の形になります\n",
        "\n",
        "        return action"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqDIylRfqDfg",
        "outputId": "fef3c389-3eef-4cc6-dda9-4e80426772c4"
      },
      "source": [
        "# torch.gather(input, dim, index)\n",
        "t = torch.tensor([[0, 1], [2, 3]])\n",
        "print(torch.gather(t, 1, torch.tensor([[0, 0],[1, 0]])))\n",
        "# print(t.gather(1, torch.tensor([[0, 0],[1, 0]])))\n",
        "# dim = 次元 ( 0は行, 1は列 )\n",
        "# index = その次元のindex。0は0番目\n",
        "# input = 解析する元のデータ。torch.tensorのインスタンスの場合はインスタンス.gather()でも動く"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0, 0],\n",
            "        [3, 2]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvoCVmTk8sNW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}